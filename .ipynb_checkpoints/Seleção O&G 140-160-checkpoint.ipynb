{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import ast\n",
    "from xml.dom import minidom\n",
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando dataframe com Vetores, Keywords e com os artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = pd.read_json(\"Elsevier_abstract - Keywords.json\")\n",
    "multi_vector = pd.read_csv(\"Elsevier_abstract - Multivector.csv\")\n",
    "consolidado = pd.read_json(\"Elsevier_abstract - Consolidado.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao ler o arquivo \"Elsevier_abstract - Multivector.csv\" os valores da coluna \"Doc2Vec\" vem como string e não como uma lista. Portanto usamos a função \"ast.literal_eval\" para converter a string em lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_vector[\"Doc2Vec\"] = multi_vector[\"Doc2Vec\"].apply(ast.literal_eval) #Convertendo string em list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após selecionar as keywords mais representativas do domínio que se busca, será identificado os artigos mais próximos dessas keywords. Primeiramente será identificado o artigo mais próximo de cada uma das keyword, em seguida seleciona-se o segundo artigo mais próximo e assim sucessivamente. Caso um artigo já tenha sido selecionado, passa-se para o artigo mais próximo que ainda não tenha sido selecionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "969"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords['Keywords'].tolist())#.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de Keywords selecionadas:  969\n"
     ]
    }
   ],
   "source": [
    "# Keywords selecionadas\n",
    "queries = keywords['Keywords'].tolist()\n",
    "\n",
    "print (\"Número de Keywords selecionadas: \", len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "new_corpus = []        # Lista com artigos selecionados\n",
    "number_article = 10    # Número de artigos por Keyword\n",
    "\n",
    "\n",
    "for m in range(number_article): # Iterando para selecionar os m artigos mais próximos de cada query\n",
    "    \n",
    "    for query in queries: # Iterando sobre cada Keyword selecionada\n",
    "        try:\n",
    "            # Realizando uma consulta de acordo com a coluna \"Description\" \n",
    "            # e retornando os N vetores mais próximos\n",
    "\n",
    "            # Indentificando o vetor da query\n",
    "            query_vector = multi_vector[multi_vector[\"Description\"] == query][\"Doc2Vec\"].values[0]\n",
    "\n",
    "            #Criando um Dataframe para a query\n",
    "            multi_vector_query = multi_vector[multi_vector[\"Type_vec\"] == \"Article\"] \n",
    "\n",
    "            #Acrescentando uma coluna com as distâncias e selecionando o N mais próximos\n",
    "            multi_vector_query[\"DisQuery\"] = (multi_vector[\"Doc2Vec\"].\n",
    "                                              apply(lambda x: distance.\n",
    "                                                    cosine(x , query_vector)))\n",
    "            multi_vector_query = multi_vector_query.sort_values(\"DisQuery\")\n",
    "\n",
    "            for indice, row in multi_vector_query.iterrows():  # Verificando os artigos mais próximos da query\n",
    "                if row[\"Description\"] not in new_corpus:       # Se o Artigo não estiver em \"new_corpus\"\n",
    "                    new_corpus.append(row[\"Description\"])      # Acrescenta em \"new_corpus\"\n",
    "                    multi_vector.drop(multi_vector.index[indice]) # e retira do multivector\n",
    "                    break                                         # e para a iteração\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Affiliate</th>\n",
       "      <th>Creator</th>\n",
       "      <th>DOI</th>\n",
       "      <th>Date</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>O&amp;G</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n               Abstract\\n               \\n  ...</td>\n",
       "      <td>Petrobras</td>\n",
       "      <td>[Melo, Marcel, Schluter, Helga, Ferreira, Jail...</td>\n",
       "      <td>10.1016/j.desal.2009.09.095</td>\n",
       "      <td>2010-01-30</td>\n",
       "      <td>[Reverse osmosis, Nanofiltration, Produced wat...</td>\n",
       "      <td>1</td>\n",
       "      <td>Advanced performance evaluation of a reverse o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\n               Abstract\\n               \\n  ...</td>\n",
       "      <td>Petrobras</td>\n",
       "      <td>[Dalmaschio, Guilherme P., Malacarne, Majorie ...</td>\n",
       "      <td>10.1016/j.fuel.2013.07.008</td>\n",
       "      <td>2014-01-31</td>\n",
       "      <td>[Petroleomics, Distillation system, Mass spect...</td>\n",
       "      <td>1</td>\n",
       "      <td>Characterization of polar compounds in a true ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>\\n               Abstract\\n               \\n  ...</td>\n",
       "      <td>Petrobras</td>\n",
       "      <td>[Soares, Ricardo R., Martins, Douglas F., Pere...</td>\n",
       "      <td>10.1016/j.molcata.2015.12.026</td>\n",
       "      <td>2016-10-31</td>\n",
       "      <td>[Glycerol gas-phase reforming, Pt/C-black cata...</td>\n",
       "      <td>1</td>\n",
       "      <td>On the gas-phase reforming of glycerol by Pt o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>\\n               Abstract\\n               \\n  ...</td>\n",
       "      <td>Petrobras</td>\n",
       "      <td>[Campos, Mario C.M., Gomes, Marcos V.C., Morei...</td>\n",
       "      <td>10.1016/S1570-7946(09)70671-6</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>[advanced control system, regulatory control, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Advanced Control and Optimization of a Natural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>\\n               Abstract\\n               \\n  ...</td>\n",
       "      <td>Shell</td>\n",
       "      <td>[Crossland, N.O., Mitchell, G.C., Bennett, D.,...</td>\n",
       "      <td>10.1016/0147-6513(91)90057-V</td>\n",
       "      <td>1991-10-31</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>An outdoor artificial stream system designed f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Abstract  Affiliate  \\\n",
       "0      \\n               Abstract\\n               \\n  ...  Petrobras   \n",
       "10     \\n               Abstract\\n               \\n  ...  Petrobras   \n",
       "100    \\n               Abstract\\n               \\n  ...  Petrobras   \n",
       "1000   \\n               Abstract\\n               \\n  ...  Petrobras   \n",
       "10000  \\n               Abstract\\n               \\n  ...      Shell   \n",
       "\n",
       "                                                 Creator  \\\n",
       "0      [Melo, Marcel, Schluter, Helga, Ferreira, Jail...   \n",
       "10     [Dalmaschio, Guilherme P., Malacarne, Majorie ...   \n",
       "100    [Soares, Ricardo R., Martins, Douglas F., Pere...   \n",
       "1000   [Campos, Mario C.M., Gomes, Marcos V.C., Morei...   \n",
       "10000  [Crossland, N.O., Mitchell, G.C., Bennett, D.,...   \n",
       "\n",
       "                                 DOI       Date  \\\n",
       "0        10.1016/j.desal.2009.09.095 2010-01-30   \n",
       "10        10.1016/j.fuel.2013.07.008 2014-01-31   \n",
       "100    10.1016/j.molcata.2015.12.026 2016-10-31   \n",
       "1000   10.1016/S1570-7946(09)70671-6 2009-12-31   \n",
       "10000   10.1016/0147-6513(91)90057-V 1991-10-31   \n",
       "\n",
       "                                                Keywords  O&G  \\\n",
       "0      [Reverse osmosis, Nanofiltration, Produced wat...    1   \n",
       "10     [Petroleomics, Distillation system, Mass spect...    1   \n",
       "100    [Glycerol gas-phase reforming, Pt/C-black cata...    1   \n",
       "1000   [advanced control system, regulatory control, ...    1   \n",
       "10000                                                 []    1   \n",
       "\n",
       "                                                   Title  \n",
       "0      Advanced performance evaluation of a reverse o...  \n",
       "10     Characterization of polar compounds in a true ...  \n",
       "100    On the gas-phase reforming of glycerol by Pt o...  \n",
       "1000   Advanced Control and Optimization of a Natural...  \n",
       "10000  An outdoor artificial stream system designed f...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consolidado['Selecionados'] = consolidado['Title'].apply(lambda x: x in new_corpus)\n",
    "novo_consolidado = consolidado[consolidado['Selecionados'] == True]\n",
    "del novo_consolidado['Selecionados']\n",
    "novo_consolidado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9694"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novo_consolidado.to_json('Elsevier_abstract - O&G.json', date_format = 'iso')\n",
    "len(novo_consolidado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando uma lista com os autores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de autores:  34020\n"
     ]
    }
   ],
   "source": [
    "# Identificando autores de O&G que parecem mais de 5 vezes\n",
    "\n",
    "# frequência mínima para um autor ser considerada\n",
    "min_freq = 1\n",
    "\n",
    "# Separando os artigos de O&G\n",
    "creator_list = novo_consolidado\n",
    "creators = []\n",
    "\n",
    "# Incluindo autor de cada artigo na lista \"creators\"\n",
    "creator_list[\"Creator\"].apply(creators.extend)   \n",
    "\n",
    "# Criando DataFrame com a frequência dos autores\n",
    "creators = pd.Series(creators)\n",
    "creators = pd.DataFrame(data={\"Frequency\": creators.value_counts()})\n",
    "\n",
    "# Retirand as keywords com Frequência menor do que min_freq\n",
    "creators = creators[creators[\"Frequency\"] >= min_freq]\n",
    "\n",
    "print (\"Número de autores: \", len(creators))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajustando o nome dos autores para refazer a busca na API da Elsevier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_creator(creator):\n",
    "    autor = creator.split(',')\n",
    "    autor = autor[1].strip() + \" \" + autor[0]\n",
    "    return(autor)\n",
    "\n",
    "new_creators = []\n",
    "\n",
    "for creator in creators.index.values:\n",
    "    new_creators.append(invert_creator(creator))\n",
    "    \n",
    "creators = sorted(new_creators)\n",
    "np.savetxt(\"Creators - O&G.csv\", creators, delimiter=\",\", fmt='%s', encoding = \"UTF-8\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identificando os Artigos escritos pelos autores listados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo configurações globais de proxy\n",
    "chave = 'upe2'\n",
    "pwd = 'fBO61299'\n",
    "proxy_url = 'http://'+chave+':'+pwd+'@inet-sys.gnet.petrobras.com.br:804/'\n",
    "proxies = {\n",
    "  'http' : proxy_url ,\n",
    "  'https' : proxy_url ,\n",
    "}\n",
    "\n",
    "api_key = \"00e1fdc3a31e50d2874b39607e13486d\"    # API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n"
     ]
    }
   ],
   "source": [
    "DOIs = []                  #Lista para armazenar os DOIs dos artigos\n",
    "n = 0                      # Contador\n",
    "\n",
    "for creator in creators:   # Iterando para cada autor da lista\n",
    "    \n",
    "    try:\n",
    "        q_creator = \"\"         # Preparando o nome do autor para usar na query da API\n",
    "        for name in creator.split():\n",
    "            q_creator = q_creator + name + \"%20\"\n",
    "        q_creator = q_creator[:-3]\n",
    "\n",
    "\n",
    "        query = 'aus(\"' + q_creator + '\")'              # Query da API\n",
    "       \n",
    "\n",
    "        # URL para requisitar os artigos de cada autor\n",
    "        start = 0\n",
    "        step =  100\n",
    "        url = (\"https://api.elsevier.com/content/search/sciencedirect?query=\" + query +\n",
    "               \"&count=\" + str(step) + \n",
    "               \"&start=\" + str(start) +\n",
    "               \"&APIKey=\" + api_key)\n",
    "        f = requests.get(url, proxies=proxies).text\n",
    "        f = json.loads(f)\n",
    "\n",
    "        # Buscando no XML os DOI dos artigos\n",
    "        n_entries = len(f[\"search-results\"][\"entry\"])\n",
    "        for entry in f[\"search-results\"][\"entry\"]:\n",
    "            try:\n",
    "                author = entry['authors']['author']\n",
    "\n",
    "                # Conferindo se o nome do Autor bate com o nome do autor da nossa lista\n",
    "                if creator in author: \n",
    "                    try:\n",
    "                        DOIs.append(entry['dc:identifier'])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                else:\n",
    "                    # Conferindo se o nome do Autor bate com o nome do autor da nossa lista\n",
    "                    for aut in author:\n",
    "                        try:\n",
    "                            if aut[\"$\"] in creator:\n",
    "                                #Incluindo DOI na lista de DOIs\n",
    "                                DOIs.append(entry['dc:identifier'])\n",
    "                                break\n",
    "                        except:\n",
    "                            pass\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    n = n +1          # Somar contador, imprimir e salvar de 500 em 500\n",
    "    if n%500 == 0:\n",
    "        print(n)\n",
    "        np.savetxt(\"DOIs O&G.csv\", DOIs, delimiter=\",\", fmt='%s', encoding = \"UTF-8\" )\n",
    "\n",
    "np.savetxt(\"DOIs O&G.csv\", DOIs, delimiter=\",\", fmt='%s', encoding = \"UTF-8\" )\n",
    "DOIs = set(DOIs)   # Retirando elementos repetidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOIs = pd.read_csv(\"DOIs O&G.csv\", header=None)\n",
    "DOIs = DOIs[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180660"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DOIs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir função reqisitar o XML de cada artigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(article):\n",
    "    \n",
    "    article = BeautifulSoup(article, \"lxml\")\n",
    "\n",
    "    try:\n",
    "        DOI = article.find(\"dc:identifier\").get_text()[4:]\n",
    "    except:\n",
    "        DOI = \"\"\n",
    "        \n",
    "    try:\n",
    "        Title = article.find(\"dc:title\").get_text()\n",
    "    except:\n",
    "        Title = \"\"\n",
    "    \n",
    "    try:\n",
    "        Date = article.find(\"prism:coverdate\").get_text()    \n",
    "    except:\n",
    "        Date = \"\"\n",
    "    \n",
    "    try:\n",
    "        Abstract = article.find(\"dc:description\").get_text()\n",
    "    except:\n",
    "        Abstract = \"\"\n",
    "    \n",
    "    try:\n",
    "        Raw_text = article.find(\"xocs:rawtext\").get_text()\n",
    "    except:\n",
    "        Raw_text = \"\"\n",
    "    \n",
    "    Keywords = []\n",
    "    Creators = []\n",
    "    try:\n",
    "        coredata = article.find(\"coredata\")\n",
    "        for data in coredata.descendants:\n",
    "            try:\n",
    "                if data.name == \"dcterms:subject\":\n",
    "                    Keywords.append(data.get_text())\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                if data.name == \"dc:creator\":\n",
    "                    Creators.append(data.get_text())\n",
    "            except:\n",
    "                pass\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    row = pd.Series([DOI, Title, Date, Abstract, Raw_text, Keywords, Creators],\n",
    "                    index=[\"DOI\",\n",
    "                           \"Title\",\n",
    "                           \"Date\",\n",
    "                           \"Abstract\",\n",
    "                           \"Raw_text\",\n",
    "                           \"Keywords\",\n",
    "                           \"Creator\"])\n",
    "            \n",
    "    return(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_abstracts(target_path, filename, DOIs):\n",
    "    articles = {\"DOI\":[],\n",
    "                \"Title\": [],\n",
    "                \"Date\": [],\n",
    "                \"Abstract\": [],\n",
    "                \"Raw_text\": [],\n",
    "                \"Keywords\": [],\n",
    "                \"Creator\": [] }\n",
    "    \n",
    "    articles = pd.DataFrame(data=articles)\n",
    "\n",
    "    print('Buscando os arquivos no site da Elsevier...')\n",
    "    print('Total de artigos: ', len(DOIs))\n",
    "    n = 70000\n",
    "    for DOI in DOIs:\n",
    "        try:\n",
    "            f = requests.get(\"https://api.elsevier.com/content/article/doi/\" + DOI + \"?APIKey=\" + api_key, proxies=proxies)\n",
    "            f = f.text\n",
    "            articles = articles.append(get_article(f), ignore_index=True)\n",
    "            n = n + 1\n",
    "            if n % 1000 == 0:\n",
    "                print ('Artigos: ', n)\n",
    "                arquivo_saida = os.path.join(target_path, filename + \" - \" + str(n))\n",
    "\n",
    "                #Gravando os artigos de 1000 em 1000\n",
    "                articles.to_json(arquivo_saida + \".json\")\n",
    "\n",
    "                # Zerando o Dataframe \"article\"\n",
    "                articles = {\"DOI\":[],\n",
    "                    \"Title\": [],\n",
    "                    \"Date\": [],\n",
    "                    \"Abstract\": [],\n",
    "                    \"Raw_text\": [],\n",
    "                    \"Keywords\": [],\n",
    "                    \"Creator\": [] }\n",
    "                articles = pd.DataFrame(data=articles)\n",
    "        except:\n",
    "            pass\n",
    "    return(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando os arquivos no site da Elsevier...\n",
      "Total de artigos:  20000\n"
     ]
    }
   ],
   "source": [
    "# Extraindo os artigos de Geociências\n",
    "target_path = '\\\\Users\\\\upe2\\\\Desktop\\\\Elsevier\\\\Tech Mining - Estratégia'\n",
    "get_abstracts(target_path, \"Novos Artigos O&G 140-160\", DOIs[140000:160000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
